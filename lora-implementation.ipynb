{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create LoRA Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRA(nn.Module):\n",
    "    def __init__(self, embed_dim, rank):\n",
    "        super().__init__()\n",
    "        self.rank = rank \n",
    "\n",
    "        # Low-rank trainable matrices\n",
    "        self.A = nn.Parameter(torch.randn(embed_dim, rank) * 0.01) # Matrix A is intialized from a Gaussian distribution; 0.01 as standard deviation\n",
    "        self.B = nn.Parameter(torch.zeros(rank, embed_dim)) # Matrix B is initialized to 0 \n",
    "\n",
    "    def forward(self, X):\n",
    "        # LoRA update: matrix multiplication\n",
    "        X = X @ self.A @ self.B\n",
    "\n",
    "        return X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement LoRA in Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRAAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, rank):\n",
    "        super().__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.rank = rank \n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        # Original frozen weights matrices (Wq, Wk, Wv, Wo)\n",
    "        self.Wq = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.Wk = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.Wv = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "        self.Wo = nn.Linear(embed_dim, embed_dim, bias=False)\n",
    "\n",
    "        # LoRA low-rank adapaters for Wq and Wv; don't modify Wk, Wo\n",
    "        self.lora_q = LoRA(embed_dim, rank)\n",
    "        self.lora_v = LoRA(embed_dim, rank)\n",
    "        \n",
    "    def forward(self, X):\n",
    "\n",
    "        # Compute query, key, value \n",
    "        Q = self.Wq(X) + self.lora_q(X) # LoRa for Q \n",
    "        K = self.Wk(X) # no LoRA for K \n",
    "        V = self.Wv(X) + self.lora_v(X) # LoRa for V \n",
    "\n",
    "        # Compute scaled dot-product attention \n",
    "        scores = (Q @ K.transpose(-2, -1)) / (self.head_dim ** 0.5)\n",
    "        attn_weights = scores.softmax(dim=-1)\n",
    "        attn_output = attn_weights @ V \n",
    "\n",
    "        # Apply output project (Wo)\n",
    "        output = self.Wo(attn_output)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement LoRA in Transformer Block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import torch \n",
    "import torch.nn as nn\n",
    "\n",
    "class LoRAEncoderTransformerBlock(nn.Module):\n",
    "    \n",
    "    def __init__(self, embed_dim, num_heads, ff_dim, rank):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define layers \n",
    "        self.lora_attention = LoRAAttention(embed_dim, num_heads, rank)\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(embed_dim, ff_dim),\n",
    "            nn.ReLU(), \n",
    "            nn.Linear(ff_dim, embed_dim)\n",
    "        )\n",
    "\n",
    "    # Forward pass\n",
    "    def forward(self, X):\n",
    "\n",
    "        # Multi-Head Attention\n",
    "        attn_output = self.lora_attention(X)\n",
    "\n",
    "        # Add & Norm\n",
    "        X = self.norm1(X + attn_output)\n",
    "\n",
    "        # Feed-forward\n",
    "        ff_output = self.ff(X)\n",
    "\n",
    "        # Add & Norm\n",
    "        X = self.norm2(X + ff_output)\n",
    "\n",
    "        return X"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement LoRA in Transformer (Encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoRASimpleTransformer(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, num_heads, ff_dim, num_layers, max_len, rank):\n",
    "        super().__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim) # input embedding\n",
    "        self.pos_embed = nn.Parameter(torch.randn(1, max_len, embed_dim)) # Positional encoding\n",
    "        self.layers = nn.ModuleList([\n",
    "            LoRAEncoderTransformerBlock(embed_dim, num_heads, ff_dim, rank) for _ in range(num_layers)\n",
    "        ])\n",
    "        self.output_layer = nn.Linear(embed_dim, vocab_size) # final output\n",
    "\n",
    "    def forward(self, X):\n",
    "        # Apply positional encoding to input embeddings\n",
    "        X = self.embed(X) + self.pos_embed[:, :X.shape[1], :]\n",
    "\n",
    "        # Nx blocks \n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "\n",
    "        # Project to vocab size for classification\n",
    "        X = self.output_layer(X)\n",
    "\n",
    "        return X "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implement function to freeze weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def freeze_original_weights(model):\n",
    "    \"\"\"\n",
    "    Freezes all non-LoRA parameters so only LoRA parameters are trainable.\n",
    "    \"\"\"\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"lora\" not in name:  # Only LoRA parameters should be trainable\n",
    "            param.requires_grad = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 7.0529\n",
      "Epoch [2/5], Loss: 7.0143\n",
      "Epoch [3/5], Loss: 6.9758\n",
      "Epoch [4/5], Loss: 6.9365\n",
      "Epoch [5/5], Loss: 6.8959\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim \n",
    "import torch.nn as nn\n",
    "\n",
    "# Model hyperparameters \n",
    "vocab_size = 1000\n",
    "embed_dim = 128\n",
    "num_heads = 4\n",
    "ff_dim = 256 \n",
    "num_layers = 2\n",
    "max_len = 20\n",
    "rank = 4 # LoRA rank \n",
    "\n",
    "# Initialize LoRA transformer\n",
    "model = LoRASimpleTransformer(vocab_size=vocab_size, embed_dim=embed_dim, num_heads=num_heads, ff_dim=ff_dim, num_layers=num_layers, max_len=max_len, rank=rank)\n",
    "\n",
    "# Freeze original Transformer weights (only train LoRA)\n",
    "freeze_original_weights(model)\n",
    "\n",
    "# Loss function & optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "\n",
    "# Dummy dataset (random tokens)\n",
    "X_train = torch.randint(0, vocab_size, (32, max_len))  # Batch of 32\n",
    "y_train = torch.randint(0, vocab_size, (32, max_len))  # Target\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    optimizer.zero_grad()\n",
    "    outputs = model(X_train)  # Forward pass\n",
    "    loss = criterion(outputs.view(-1, vocab_size), y_train.view(-1))  # Compute loss\n",
    "    loss.backward()  # Backpropagation\n",
    "    optimizer.step()  # Update only LoRA parameters\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pediatric_arrivals",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
